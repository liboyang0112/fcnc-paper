
Dear reviewer,

Thanks for your useful comments, which helped to improve the clarity of the paper.
Please find our replies below, starting by “——>”.

Best regards,

The ATLAS Collaboration                       
            


The paper describes a search for rare decays of top quarks to Hc and Hu final states, exploiting fermionic decays of the Higgs boson. This result complements previous searches for the same rare decays in other Higgs boson’s final states. A combination with these other searches is also presented.

The paper is well written. The analysis is documented in great details. The strategy is robust, based on established procedures for similar searches. Reading the document, I noted down a set of questions that I would like the authors to address, before I could make any recommendation.

It is clear that the analysis has no sensitivity to the SM expectation for these final states. As the paper discusses, one could observe a signal excess thanks to a modification of the Higgs sector. On the other hand, the analysis, and in particular the combination, assumes SM-like BRs for the Higgs boson. In this respect, the quoted combined upper limit can be interpreted only in the context of the SM. I would have expected a study of the impact of this measurement on some of the BSM scenarios, to give a sense of the impact of the result. Without it, it is complicated to understand if the analysis meets the requirements of a JHEP publication.    
----> This search considers a simplified model where the only free parameter is the non-flavour-diagonal Yukawa coupling between the top quark and the SM Higgs boson. The flavour-diagonal couplings of the SM Higgs boson are assumed to be as predicted by the SM. It is customary to make this assumption in the interpretation of these searches, especially since the Higgs (flavour-diagonal) coupling measurements at the LHC are indeed consistent with the SM within O(10%) (see e.g. Fig. 11 in http://cdsweb.cern.ch/record/2629412). Furthermore, beyond-SM (BSM) scenarios that can predict a t->Hq decay at the level of the current sensitivity typically predict modifications to the Higgs BRs at the few percent level or below, well beyond the current precision of LHC Higgs measurements. In any case, the quoted results would not be significantly affected by even a 10% change in the assumed BRs: the worst-case scenario would be a coherent change in all visible decay modes, e.g. due to the presence of an invisible decay mode, and this would mean only a 10% change in the best-fit BR(t->Hq), well below the uncertainty of O(100%) obtained from the fit. Therefore, we believe the assumption of SM Higgs BRs, which is clearly stated, is well motivated and does not invalidate the applicability of the bounds obtained to a broad class of BSM scenarios. 

In relation to the previous point, the comparison of the UL derived in this search with those previously published clearly shows that the old results dominate the average. One would then need to understand the impact of the new results in a more quantitative way, as a way to address the relevance of the presented measurements.    
----> Previous Run 2 searches for t->Hq were focused on the H->gamgam and WW decay modes. The primary goal of this paper is to extend the search to include the main fermionic decay modes of the Higgs boson (H->bb and tautau). The t->Hq, H->tautau search is performed for the first time at the LHC, and is found to have sensitivity competitive to that of the H->gamgam and WW searches. The t->Hq, H->bb search is found to be limited by systematic uncertainties, and publishing these results that documenting those limitations will help in improving future similar searches in this decay mode. We believe the comparison of results across different search modes is extremely important, both to assess the overall consistency of the results obtained, as well as to improve the sensitivity through their combination. By combining all searches, the expected sensitivity is improved by about a factor of two relative to the most sensitive published individual searches. The results obtained represent the most restrictive direct bounds available to date. We believe this paper presents very relevant results that merit publication in JHEP. 

It is not clear from the discussion of the SM background generation, as well as in the systematic uncertainties evaluation, how realistic is the modeling of gluon splitting in tt events, which provide a source of tt+b events. In general, MC generators tend to underestimate this process. Could this be problematic?
----> The modeling of the ttbb background is an aspect of the analysis that a-priori requires some care. Therefore, the most detailed treatment of this background in ATLAS, as developed for the ttH(bb) search (arXiv:1712.08895), has been used in this analysis. In the paper we refer to this publication for further details. In the fit the normalisation of this background is free floating (the fitted scale factor is 1.17 +/- 0.15, i.e. consistent with the nominal MC prediction) and a number of modeling uncertainties are considered. However, the ttbb background is not the main limiting factor in the analysis. As discussed in Sect. 10.1, the largest systematic uncertainties are associated with the tt+light-jets background, from modeling uncertainties (parton shower and hadronisation model variations), as well as from the limited precision in the charm mistag rate calibration (a charm mistag is the way tt+light-jet events enter the 3b sample).

At page 17, how is the sensitivity of the quantities entering the BDT evaluated?
----> The sensitivity performance of the variables entering the BDT is quantified by the “separation” and “importance” measures provided by the TMVA package (see arXiv:physics/0703039). The separation is 0 (1) for identical (non-overlapping) signal and background shapes. The importance is evaluated variable separation times the fraction of times a given variable is used to apply cuts in the BDT nodes. Added a sentence clarifying this point.

On Fig.5, the region 40 < mττ < 80 GeV, which is a bkg-only region, seem to be incorrectly modelled. What is the point in having this region included, while it could potentially bias the result?
----> The low mττ region indeed shows a slight discrepancy between data and prediction, which was also observed in the dedicated control region for the fake τhad background. The difference observed in this control region was assigned as a systematic uncertainty to the fake τhad background in the signal region. The number of background of events affected represent a small fraction of the total and are far from where the signal is expected. In general, for a BDT analysis it is better to not apply cuts on the input variables and allow the decision tree and background fit to define the discrimination. As shown in Fig. 10, the BDT output distribution is well modeled both before and after the fit. Also, as indicated in Sect. 9, one of the tests performed was to verify that the fit is able to correctly determine the strength of a simulated signal injected into the real data. Therefore, no significant bias was expected nor observed. Since a possible bias due to this mismodelling is well under control there was no reason to apply a cut on the variable. 

The authors add a systematic uncertainty to take into account that b-tagging is used beyond the kinematic region in which the data-MC scale factors are derived. How is this systematic uncertainty evaluated?
----> This uncertainty is evaluated in the simulation by comparing the tagging efficiencies while varying e.g. the fraction of tracks with shared hits in the silicon detectors or the fraction of fake tracks resulting from random combinations of hits, both of which typically increase at high pT due to growing track multiplicity and density of hits within the jet. However, the impact of this uncertainty in the analysis is negligible, because of the very small fraction of signal and background events falling in such kinematic regime. A clarification has been added in the last paragraph of Sect. 8.2

The post-fit uncertainties of both the analyses are substantially smaller than those of the pre-fit configuration. On the other hand, so significant pull of the nuisance parameter is observed. Are these pulls evaluated using the pre-fit uncertainty? Is the reduction ”typical”/expected? Did you verify (e.g., with toy MC tests) that this flexibility of the fit to adapt to differences between data and pre-fit expectation would not cause a bias to the fit, in presence of a signal?
----> The pulls are in units of the pre-fit uncertainty. The reduction of the total postfit uncertainty is as expected from studies using mock data, and primarily results from anticorrelations between nuisance parameters that are built by the fit when considering high-statistics signal-depleted regions. Indeed it was verified that the fit has sufficient degrees of freedom to adjust discrepancies between data and pre-fit background without biasing the signal extraction. The tests performed are summarized in the 3rd paragraph of Sect. 9.

On page 29, the results are said to be ”dominated by statistics”. Is this a way to say that the dominant source of uncertainty is the statistical one?
----> Yes. This can be appreciated from the reported statistical and systematic uncertainties for the individual searches in Figs. 11 and 12.

In the Appendix, it would be much easier if the authors could put the pre-fit and post-fit results next to each other.
----> Including both tables in the same page, or merging them so that prefit and postfit yields are shown next to each would make a very busy page. We believe the current presentation is acceptable and would prefer to keep it as is.

I am puzzled by the fact that the post-fit uncertainties (stat + sys in quadrature) are generically much larger than the Poisson uncertainty (as they should be, in presence of systematics, nuisance, etc.), but this is not the case for ttH in the 4j and 5j signal regions. Do you have an explanation of this?
----> When using a profile likelihood fit, the postfit uncertainties in regions with high statistics, e.g. 3b regions, are expected to be comparable to the Poisson uncertainties in the data (even in presence of systematic uncertainties), since those bins drive the fit. In the 4b regions, which have lower statistics per bin, and where some systematic uncertainties have a larger impact than in the 3b regions, the postfit uncertainty can be larger than the Poisson uncertainty. The 2b regions also contribute to the constraining of uncertainties, particularly those related to tt+light jet modeling, and 4j2b and 5j2b regions have higher statistics than the 6j2b region. It should also be pointed out that at higher jet and b-tag multiplicity the impact from limited MC statistics is higher, and those MC statistical uncertainties, which are included in the postfit uncertainty band, are not constrained by the fit.

In the bibliography, some of the theory papers describing possible enhancement in this final state is quite old. Are you sure that there is no newer measurement (e.g., from B factories) constraining the parameter space of the 2HDM and consequently reducing the expected enhancement?            
----> Indirect constraints from precision measurements (including from B-physics) are very model dependent and often can be easily avoided. We have added additional references to more recent theory papers. The range of predicted/allowed branching ratios remain the same: up to ~10^-3 for Type-III 2HDM, and ~10^-5 for other models.

